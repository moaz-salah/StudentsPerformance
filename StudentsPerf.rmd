---
title: "Students Performance Project"
author: "Moaz M. Salaheldeen"
date: "2023-11-28"
output: pdf_document
---

# Introduction  

## Dataset

This dataset shows the Marks secured by students in different subjects, with other information about each student's behavior (Kind of lunch they have, test preparation course completion) in addition to information related to gender, race and parent's education level. 
Dataset is available for public in  [**Kaggle**](https://www.Kaggle.com)

**Why did I choose this Dataset?**  

One of the main reasons I chose this dataset is its size, it is relatively small. This is clear from the size of the source data file (one csv file -less than 10KB). And also proved to be true when seeing that the number of records is 1000 records only. 
While this is surely very limited number of records in real life, it perfectly fits our goals in a project with learning objectives. This size allows trying different functions and models that couldn't be executed on a personal laptop with a larger datasets -like MovieLens 10 Million Records dataset-.      

## Goals of the Project
The aim of this project is to explore Students Performance dataset. Our work will focus on trying to find out which and how of these factors affect the students results, and then will try to build a model that participate exam scores for a student given these factors.

To do so, we will do these steps:

* Data Importing and cleansing
* Data Exploring and Visualization
* Modeling
* Results and Conclusions


# Step 1: Data  Importing and Cleansing   
```{r definitionChunk,  echo = TRUE}
studentsDF<- read.csv('StudentsPerformance.csv') #File must be under WD
```
Dataset consists of 1000 record, 8 columns (5 related to properties associated with students and 3 test results:Math, reading and writing). The data is complete, no missing data in any column/value. All columns are of type 'Char'. We can see that the 5 properties are Categorical, while the scores are numeric values between 0 and 100.(Actually only in Math we have a Zero score, but it is logical to assume that 0 is possible exam mark) 


# Step 2: Data Visualization (& Insights)

**Check relation between the three scores**  

First, We'll examine the three scores and check if there is any kind of relation between them
(Note: for more readability I renamed the columns names to shorter names)
```{r OP_cor_1_Chunk, include = FALSE}
library(tidyverse)
library(caret)
library(ggpubr)
library(MASS)
library(rpart)
library(randomForest)

#change the column names to smaller, more readable names
studentsDF<- studentsDF|> rename("race" = "race.ethnicity"  ,
      "P_education" ="parental.level.of.education" ,
      "Test_Prepare"="test.preparation.course",
      "math" ="math.score"  ,
      "reading" = "reading.score" ,
      "writing" ="writing.score" )

r_w_plot<- ggplot(studentsDF, aes(x=reading, y=writing)) + 
  geom_point()+
  geom_smooth(method=lm)
r_m_plot<- ggplot(studentsDF, aes(x=reading, y=math)) + 
  geom_point()+
  geom_smooth(method=lm)
w_m_plot<- ggplot(studentsDF, aes(x=writing, y=math)) + 
  geom_point()+
  geom_smooth(method=lm)


read_write_relation<- cor.test(studentsDF$reading, studentsDF$writing, 
                               method = "pearson")

read_math_relation<- cor.test(studentsDF$reading, studentsDF$math, 
                              method = "pearson")

write_math_relation<- cor.test(studentsDF$writing, studentsDF$math, 
                               method = "pearson")
  
```

```{r OP_cor_2_Chunk, echo=FALSE}
ggarrange(r_w_plot, r_m_plot, w_m_plot, 
          labels = c("A", "B", "C"),
          ncol = 2, nrow = 2)

read_write_relation     
read_math_relation     
write_math_relation  
```

As seen clearly in the diagrams there is a linear relation between each two subjects. This is also confirmed by the Pearson correlation factor,   

$r = \frac{N\sum{XY}-(\sum{X}\sum{Y})}{\sqrt{ [N \sum{x^2}-(\sum{x})^2 ][N \sum{y^2}-(\sum{y})^2 }]}$  

This indicates there is High correlation between all subjects, slightly -and as expected logically- higher between "reading" and "writing"; 0.9545981 between "reading" & "writing", compared to 0.8175797 and 0.802642 between "Math" and each of "reading" and "writing" respectively.

To simplify our work, we will benefit from this high correlation and create new variable at the dataset that represent the average of the three subjects

```{r Avg_Score}
studentsDF<- studentsDF |> mutate(avg_score=(math+ reading + writing)/3)
```


**Check distribution of Inputs**  

When we check "Gender" column of the data to see if there is a dominant value, we can see that   
```{r Gender_Dist, , echo = FALSE}
studentsDF |> group_by(gender) %>%
  summarise(n=n()) %>%
  arrange(n) %>%
  ggplot(aes(x=gender,y=n)) +
  geom_bar(stat="identity") +
  geom_text(aes(label=n), vjust=-0.3, size=3.5)+
  ggtitle("Distribution of Gender") +
  xlab("Gender") +
  ylab("Number in Gender Group") 

studentsDF |> group_by(race, gender) %>%
  summarise(n=n()) %>%
  #  arrange(n) %>%
  ggplot(aes(x=race,y=n, fill=gender)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=n), vjust=-0.3, position = position_dodge(0.9), size=3.5)+
  ggtitle("Distribution of Gender per Race") +
  xlab("Race") +
  ylab("Number in Race Group") 
```

There are little bit "Females" than "Males", little bit more than the half (518), and the percentage of females to males in each "race" doesn't show dominant gender (Males was around 43.5% in their lowest percentage - in group C and females were 40.44% in their lowest percentage - in group A).

**Check RACE effect**   

Now, it is worth checking if "race" affect other factors, for example if it affect lunch, parent education level or taking prep exam -due to possible economical differences between different group of races-

```{r race_analysis, , echo = FALSE}
studentsDF |> group_by(race, lunch) %>%
  summarise(n=n()) %>%
  #  arrange(n) %>%
  ggplot(aes(x=race,y=n, fill=lunch)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=n), vjust=-0.3, position = position_dodge(0.9), size=3.5)+
  ggtitle("Distribution of Lunch per Race") +
  xlab("Race") +
  ylab("Number in Race Group") 

studentsDF |> group_by(race, P_education) %>%
  summarise(n=n()) %>%
  #  arrange(n) %>%
  ggplot(aes(x=race,y=n, fill=P_education)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=n), vjust=-0.3, position = position_dodge(0.9), size=3.5)+
  ggtitle("Distribution of Parents Education per Race") +
  xlab("Race") +
  ylab("Number in Race Group") 

studentsDF |> group_by(race, Test_Prepare) %>%
  summarise(n=n()) %>%
  #  arrange(n) %>%
  ggplot(aes(x=race,y=n, fill=Test_Prepare)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=n), vjust=-0.3, position = position_dodge(0.9), size=3.5)+
  ggtitle("Distribution of Test_Prepare per Race") +
  xlab("Race") +
  ylab("Number in Race Group") 

```

Looking at these diagrams, we can see that more Students in all races get 'standard' type of lunch than 'free/reduced' lunch, and the percentage doesn't differ much from a group to another (it ranges from 59.6% in group A as the lowest percentage to 70.7% in group E as the highest).


Regarding the Educational level, it seems also not highly affected by the race, but as it not very clear visually, let's group the Educational level into two Main groups: 'Has degree' and 'No degree'.
```{r degree_grouped}
studentsDF |> group_by(race) %>% summarise(Number_in_Grp = n(), 
                            has_degree = sum(str_count(P_education,"degree")>0, na.rm = TRUE), 
                            degree_Perc =has_degree/Number_in_Grp )

```

It confirms the previous statements as the number of students with parents holding a degree ranges from 32.6% in group A as the lowest to 46.4% in group E as the highest- **Again**.  

Preparing Exam feature gave similar results. In all groups the number of students completed the exam ranges from 31.3% in group D -as the lowest percentage- to 42.9% in group E as the highest  **Again**.

Another check to confirm the previous results, is to apply the $\chi^2$ test 

```{r chisq_test}
chisq.test(as.factor(studentsDF$race), as.factor(studentsDF$lunch))  
chisq.test(as.factor(studentsDF$race), as.factor(studentsDF$P_education)) 
chisq.test(as.factor(studentsDF$race), as.factor(studentsDF$Test_Prepare)) 
```

All p-values are larger than 5%, so we cant reject Null hypothesis and we cant say there is a direct relation. This match the visual results. 
Still, note that the smallest p-Value is for Educational level feature much less for Educational Levels.
  
    
**Check GENDER effect**   
    

Now, we check for a probability that a certain gender may be more committed to taking preparation exam or having 'standard lunch'. 

```{r gender_analysis, echo=FALSE}
#checking if gender affect the type of Lunch
studentsDF |> group_by(gender, lunch) %>%
  summarise(n=n()) %>%
  #  arrange(n) %>%
  ggplot(aes(x=gender,y=n, fill=lunch)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=n), vjust=-0.3, position = position_dodge(0.9), size=3.5)+
  ggtitle("Distribution of lunch per Gender") +
  xlab("Gender") +
  ylab("Number in Gender Group") 

#if Gender affect taking the preparation course 
studentsDF |> group_by(gender, Test_Prepare) %>%
  summarise(n=n()) %>%
  #  arrange(n) %>%
  ggplot(aes(x=gender,y=n, fill=Test_Prepare)) +
  geom_bar(stat="identity", position=position_dodge()) +
  geom_text(aes(label=n), vjust=-0.3, position = position_dodge(0.9), size=3.5)+
  ggtitle("Distribution of Test_Prepare per Gender") +
  xlab("Gender") +
  ylab("Number in Gender Group") 
```


We see that the behavior and percentage is very similar, so we cant say that gender has relation with any of these two factors.
The high p-Value resulted from running  $\chi^2$ test, confirms the same
```{r gender_analysis_chi, echo=FALSE}
chisq.test(as.factor(studentsDF$gender), as.factor(studentsDF$lunch))  #p-value = 0.5421
chisq.test(as.factor(studentsDF$gender), as.factor(studentsDF$Test_Prepare))  #p-value = 0.9008
```


**Check Factors effect on Average Score**   

It is time now to check the relation between each feature and the Average score, that represents the three subjects' scores  
```{r input_output, echo=FALSE}
raceMedian <- studentsDF |> group_by(race) |>  summarise( MD = round(median(avg_score)))
studentsDF |> 
  ggplot(aes(x=race,y=avg_score,  vjust = -1.5)) +
  geom_boxplot() +
  geom_text(data = raceMedian, aes(as.factor(race), MD, label = MD), position = position_dodge(width = 0.8), size = 3, vjust = -0.5)+
  ggtitle("Race affect on Score") +
  xlab("Race") +
  ylab("Average Score") 



#Checking gender correlation with Avg score
genderMedian <- studentsDF |> group_by(gender) |>  summarise( MD = round(median(avg_score)))
studentsDF |> 
  ggplot(aes(x=gender,y=avg_score)) +
  geom_boxplot() +
  geom_text(data = genderMedian, aes(gender, MD, label = MD), position = position_dodge(width = 0.8), size = 3, vjust = -0.5)+
  ggtitle("Gender affect on Score") +
  xlab("Gender") +
  ylab("Average Score") 


#Checking lunch correlation with Avg score
lunchMedian <- studentsDF |> group_by(lunch) |>  summarise( MD = round(median(avg_score)))
studentsDF |> 
  ggplot(aes(x=lunch,y=avg_score)) +
  geom_boxplot() +
  geom_text(data = lunchMedian, aes(lunch, MD, label = MD), position = position_dodge(width = 0.8), size = 3, vjust = -0.5)+
  ggtitle("Lunch affect on Score") +
  xlab("Lunch") +
  ylab("Average Score") 


#Checking Education correlation with Avg score
EducMedian <- studentsDF |> group_by(P_education) |>  summarise( MD = round(median(avg_score)))
studentsDF |> 
  ggplot(aes(x=P_education,y=avg_score)) +
  geom_boxplot() +
  geom_text(data = EducMedian, aes(P_education, MD, label = MD), position = position_dodge(width = 0.8), size = 3, vjust = -0.5)+
  ggtitle("Parent Education affect on Score") +
  xlab("Parent Education" ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  ylab("Average Score") 


#Checking Test Prepar correlation with Avg score
testPrepMedian <- studentsDF |> group_by(Test_Prepare) |>  summarise( MD = round(median(avg_score)))
studentsDF |> 
  ggplot(aes(x=Test_Prepare,y=avg_score)) +
  geom_boxplot() +
  geom_text(data = testPrepMedian, aes(Test_Prepare, MD, label = MD), position = position_dodge(width = 0.8), size = 3, vjust = -0.5)+
  ggtitle("Test Preparation affect on Score") +
  xlab("Test Preparation") +
  ylab("Average Score") 

```
**Race:** Apparently, it has some affect on the results. Group A is the smallest median with only 61, while group E is the one with the highest median 74. This comes in compliance with the Analysis of input features, where this group was highest in almost all features.    

**Gender:** Females students are doing little bit higher in Median than male students (70 compared to 66). Diagram shows also they are less deviated from the median  


**Lunch:** Students having standard lunch do better in Exams. That can be seen easily from the diagram comparing the medians; 71 to 63. Diagram shows also they are less deviated from the median  


**Degree:** Students with Parents holding degrees tend to do little bit higher than others, but this factor has the weakest impact on results. Their medians were 70, 71, 73 for associate's degree, bachelor's degree and master degree respectively. While others ranged from 65 to 69.   


**Test Preparation:** Students completed Test Preparation do better in Exams. Their medians are 74 compared to 65 for those who didn't. This is the second strongest factor after Lunch.

Correlation equations shows the same results: 
```{r cor_ip_op, include=FALSE}
cor.test(as.numeric(as.factor(studentsDF$race)), studentsDF$avg_score, method = "pearson")  #cor 0.1851684  
cor.test(as.numeric(as.factor(studentsDF$gender)), studentsDF$avg_score, method = "pearson")  #cor -0.1308612  
cor.test(as.numeric(as.factor(studentsDF$lunch)), studentsDF$avg_score, method = "pearson")  #cor 0.290064   
cor.test(as.numeric(as.factor(studentsDF$P_education)), studentsDF$avg_score, method = "pearson")  #cor 0.07888339    
cor.test(as.numeric(as.factor(studentsDF$Test_Prepare)), studentsDF$avg_score, method = "pearson")  #cor 0.2567097  

```

Factor               | Correlation 
---------------------|----------
Lunch                | 29%
Test Preparation     | 25%
race                 | 18%    
gender               | 13%
Parent Education     | 7%



# Step 3:  Data Preparation and Modeling
```{r split_chunck, include=FALSE}
set.seed (1, sample.kind = "Rounding")
final_test_index <- createDataPartition(y = studentsDF$avg_score, times = 1, p = 0.2, list = FALSE)
final_test_data <- studentsDF[final_test_index,]
temp_data <- studentsDF[-final_test_index,]

work_test_index <- createDataPartition(y = temp_data$avg_score, times = 1, p = 0.2, list = FALSE)
work_test_data <- temp_data[work_test_index,]
work_train_data <- temp_data[-work_test_index,]
rm(temp_data)


##MMS: 2- Prepare Evaluation Function. Used Method is RMSE
Eval_RMSE <- function(true_ratings, predicted_ratings){    sqrt(mean((true_ratings - predicted_ratings)^2))}

##MMS: 3- Prepare Reference Model: THE AVERAGE
mu <- mean(work_train_data$avg_score, na.rm = TRUE)
mu     #67.89116

Ref_rmse <- Eval_RMSE(work_test_data$avg_score, mu)
Ref_rmse   # 13.71996

rmse_results <- tibble(method = "Just the average", RMSE = Ref_rmse)

```

## Data Preparation

20% of the dataset will be kept for the Evaluation of the final model. 20% of the remaining is considered to be the test set and the other 80% is the training set.

The Evaluation function used to compare models is the standard RMSE: 
$RMSE = \sqrt{\frac{1}{n}\Sigma_{i=1}^{n}{\Big({d_i -f_i})^2}}$

## Modeling

As all factors are Categorical, we'll try Regression Tree model. I tried two the regression trees two times; One based on the most significant factors and the other is based on all factors  

**First:**
The model we build here is based on the most significant factors: lunch, Test_Prepare, and race  
```{r regTree_top_a_chunk, echo=FALSE}
fit_reg_tree <- rpart(avg_score ~ lunch+Test_Prepare+race, data = work_train_data) 
print(fit_reg_tree)
plot(fit_reg_tree, margin = 0.1)
text(fit_reg_tree, use.n=TRUE, cex = 0.75)


```
As can be seen from the tree, the effective factors are Lunch and Test_Preparation. Race had no place in the tree, indicating its effect is implicit inside the previous two.   

I applied Cross Validation concept to reach the best parameters. I omitted Race to make it simpler 
```{r regTree_top_b_chunk}
train_rpart_top <- train(avg_score ~ lunch+Test_Prepare,
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 50)),
                     data = work_train_data)
plot(train_rpart_top)
train_rpart_top$bestTune
```

Now, applying this model on the test data, and adding to the table where we keep different models results, we can see very slight enhancement:
```{r regTree_top_c_chunk, include=FALSE}
y_hat <- predict(train_rpart_top, work_test_data)
Class_tree_top_rmse <- Eval_RMSE(work_test_data$avg_score, y_hat)
rmse_results<- rmse_results %>% add_row(method ="Regression Tree- Top Factors: ", RMSE = Class_tree_top_rmse) #13.2
```
```{r regTree_top_d_chunk, echo=FALSE}
rmse_results
```

**Second:**

Now we build a module after adding the other two factors; Gender and Parent Education, and repeat the above steps
```{r regTree_all_a_chunk, include=FALSE}
train_rpart_all <- train(avg_score ~ lunch+Test_Prepare+gender+P_education+ race,
                       method = "rpart",
                       tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 50)),
                       data = work_train_data)
plot(train_rpart_all)
train_rpart_all$bestTune

y_hat <- predict(train_rpart_all, work_test_data)
Class_tree_all_rmse <- Eval_RMSE(work_test_data$avg_score, y_hat)

rmse_results<- rmse_results %>% add_row(method ="Regression Tree- All Factors: ", RMSE = Class_tree_all_rmse) #13.1
```
```{r regTree_all_b_chunk, echo=FALSE}
rmse_results
```

These factors didntnt enhance the model.

In order to reach better results, I tried other models.


**RandomForest**

We tried the Random Forest technique, using all factors, and applied the results to the Evaluation Function.

```{r rf_a_1_chunk,include=FALSE}
fit_rf <- randomForest(avg_score~lunch+Test_Prepare+gender+P_education+ race, data = work_train_data) 

y_hat_rf <- predict(fit_rf, work_test_data)

RF_rmse <- Eval_RMSE(work_test_data$avg_score, y_hat_rf)
rmse_results<- rmse_results %>% add_row(method ="Random Forest ", RMSE = RF_rmse)
```

```{r rf_a_2_chunk,echo=FALSE}
rmse_results 

```


**Knn**


To apply KNN model, we will first apply Cross Validation to get the best parameter and then apply the model
```{r knn_a_chunk, include=FALSE}
train_knn <- train(avg_score~lunch+Test_Prepare+gender+P_education+ race, method = "knn",
                   data = work_train_data, 
                   tuneGrid = data.frame(k = seq(4, 70, 1)))          

y_hat_knn <- predict(train_knn, work_test_data)
knn_rmse <- Eval_RMSE(work_test_data$avg_score, y_hat_knn)

rmse_results<- rmse_results %>% add_row(method ="KNN", RMSE = knn_rmse)
```

```{r knn_b_chunk, echo=FALSE}
train_knn$bestTune


rmse_results

```



Having built three models, will **Ensemble**  the models together. In our case, ensemble can be done by just taking the average.
I'll take the average between the best two models: KNN and Random Forest 
```{r ensemble_1_chunk}
y_hat_av<- ( y_hat_rf + y_hat_knn)/2
```
```{r  ensemble_2_chunk, include=FALSE}
av_rmse <- Eval_RMSE(work_test_data$avg_score, y_hat_av)
rmse_results<- rmse_results %>% add_row(method ="Average", RMSE = av_rmse)
```

This gives us these final results
```{r  ensemble_3_chunk, echo=FALSE}
rmse_results
```


# Step 4: Results & Conclusion  (The table)
For final Evaluation for the model, we applied the Ensemble model on the 20% of the dataset we split in the beginning, and calculated the RMSE   
```{r results_chunk,include=FALSE}
y_hat_final_rf <- predict(fit_rf, final_test_data)
y_hat_final_knn <- predict(train_knn, final_test_data)

y_hat_final_av  <- ( y_hat_final_rf + y_hat_final_knn)/2

final_rmse <- Eval_RMSE(final_test_data$avg_score, y_hat_final_rf)

```

```{r results_b_chunk,echo=FALSE}
final_rmse #
```


As expected, This gave higher RMSE (14.38).




# Appendix: What if we removed OutLliars from the dataset before starting work
When we plotted the exam results for each subject (Box- plot), there were outliars in each one. We ignored this fact on the previous work, and worked on all data. But what if we try the same models but after removing these points from the dataset, to see if removing outliars has any effect on results.

Let's start by defining the outliars points in each diagram and removed the duplicate
```{r no_outliars_chunk}
outR<-boxplot.stats(studentsDF$reading)$out
outR_ind <- which(studentsDF$reading %in% c(outR))

outW<-boxplot.stats(studentsDF$writing)$out
outW_ind <- which(studentsDF$writing %in% c(outW))

outM<- boxplot.stats(studentsDF$math)$out
outM_ind <- which(studentsDF$math %in% c(outM))

Indexes_to_remove <- (outR_ind)
Indexes_to_remove <- Indexes_to_remove |> append (outW_ind)|> append (outM_ind)
Indexes_to_remove <-Indexes_to_remove[!duplicated(Indexes_to_remove)]
Indexes_to_remove
```

Then, we will eliminate these points (12 points) from the original dataset then do the same partitioning as before.
```{r no_outliars_2_chunk, include=FALSE}
set.seed (1, sample.kind = "Rounding")
studentsDF_No_OL <- studentsDF |> filter(!row_number() %in% Indexes_to_remove) 
```

```{r no_outliars_3_chunk, include=FALSE}
final_test_index_No_OL <- createDataPartition(y = studentsDF_No_OL$avg_score, times = 1, p = 0.2, list = FALSE)
final_test_data_No_OL <- studentsDF_No_OL[final_test_index_No_OL,]
temp_data <- studentsDF_No_OL[-final_test_index_No_OL,]

work_test_index_No_OL <- createDataPartition(y = temp_data$avg_score, times = 1, p = 0.2, list = FALSE)
work_test_data_No_OL <- temp_data[work_test_index_No_OL,]
work_train_data_No_OL <- temp_data[-work_test_index_No_OL,]
rm(temp_data)
```

So, the dataset we work on become distributed as follows: Final Test, Working Test and Training 
```{r no_outliars_4_chunk, echo=FALSE}
dim(final_test_data_No_OL)
dim(work_test_data_No_OL)
dim(work_train_data_No_OL)
```
Total of 988 points, which are the 1000 minus the 12 out-liars.

Applying the same three modeling techniques we used,
```{r no_outliars_5_chunk,include=FALSE}
train_rpart_N <- train(avg_score ~  lunch+Test_Prepare+gender+P_education,
                         method = "rpart",
                         tuneGrid = data.frame(cp = seq(0.0, 0.1, len = 50)),
                         data = work_train_data_No_OL)
y_hatX <- predict(train_rpart_N, work_test_data_No_OL)
reg_rmse_No_OL <- Eval_RMSE(work_test_data_No_OL$avg_score, y_hatX)  


fit_rf_N <- randomForest(avg_score~lunch+Test_Prepare+gender+P_education+ race, data = work_train_data_No_OL) 

y_hat_rf_N <- predict(fit_rf_N, work_test_data_No_OL)
rf_rmse_No_OL <- Eval_RMSE(work_test_data_No_OL$avg_score, y_hat_rf_N)   

train_knn_N <- train(avg_score~lunch+Test_Prepare+gender+P_education+ race, method = "knn",
                   data = work_train_data_No_OL, 
                   tuneGrid = data.frame(k = seq(4, 70, 1)))           
y_hat_knn_N <- predict(train_knn_N, work_test_data_No_OL)
knn_rmse_No_OL <- Eval_RMSE(work_test_data_No_OL$avg_score, y_hat_knn_N)   


mu_NO_OL <- mean(work_train_data_No_OL$avg_score, na.rm = TRUE)

Ref_rmse_NO_OL <- Eval_RMSE(work_test_data_No_OL$avg_score, mu_NO_OL)
rmse_results_NO_OL <- tibble(method = "Just the average, No Out Liars", RMSE = Ref_rmse_NO_OL)  



rmse_results_NO_OL<- rmse_results_NO_OL %>% add_row(method ="Regression tree, No Out Liars", RMSE = reg_rmse_No_OL)
rmse_results_NO_OL<- rmse_results_NO_OL %>% add_row(method ="Random Forest, No Out Liars", RMSE = rf_rmse_No_OL)
rmse_results_NO_OL<- rmse_results_NO_OL %>% add_row(method ="KNN, No Out Liars", RMSE = knn_rmse_No_OL)

y_hat_av_No_OL<- ( y_hat_rf_N + y_hat_knn_N)/2

av_rmse_NO_OL <- Eval_RMSE(work_test_data$avg_score, y_hat_av_No_OL)
rmse_results_NO_OL<- rmse_results_NO_OL %>% add_row(method ="Average-No OL", RMSE = av_rmse_NO_OL)
```
And the results are added to the table:
```{r no_outliars_6_chunk,echo=FALSE}

rmse_results_NO_OL
```

And the result of the final testing data is :
```{r  no_outliars_7_chunk,echo=FALSE}
y_hat_final_knn_NO_OL <- predict(train_knn_N, final_test_data_No_OL)

final_rmse_NO_OL <- Eval_RMSE(final_test_data_No_OL$avg_score, y_hat_final_knn_NO_OL)

```
```{r final_rmse_NO_OL}
final_rmse_NO_OL
```

As can be seen, results after removing outliars are better than results from data containing outliars





# References
The dataset can be downloaded from this link: https://www.kaggle.com/datasets/spscientist/students-performance-in-exams 


